{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: Develop an automatic classification engine for consumer goods.\n",
    "*Pierre-eloi Ragetly*\n",
    "\n",
    "This project is part of the Data Scientist path proposed by OpenClassrooms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:56.827846Z",
     "start_time": "2020-11-05T17:35:55.923935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(89)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams.update({'axes.titleweight': 'bold',\n",
    "                     'axes.titlesize': 16,\n",
    "                     'axes.labelsize': 14,\n",
    "                     'xtick.labelsize': 12,\n",
    "                     'ytick.labelsize': 12})\n",
    "\n",
    "# Where to save the figures\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    folder_path = os.path.join(\"charts\")\n",
    "    if not os.path.isdir(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    path = os.path.join(\"charts\", fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-processing\" data-toc-modified-id=\"Text-processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Remove-numbers\" data-toc-modified-id=\"Remove-numbers-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Remove numbers</a></span></li><li><span><a href=\"#Lower-casing\" data-toc-modified-id=\"Lower-casing-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Lower casing</a></span></li><li><span><a href=\"#Stop-words-removal\" data-toc-modified-id=\"Stop-words-removal-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Stop words removal</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Lemmatization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:56.850322Z",
     "start_time": "2020-11-05T17:35:56.830112Z"
    }
   },
   "outputs": [],
   "source": [
    "data = (pd.read_csv(\"data/Flipkart/flipkart_com-ecommerce_sample_1050.csv\")\n",
    "          .set_index('uniq_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:56.861419Z",
     "start_time": "2020-11-05T17:35:56.853344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1050 entries, 55b85ea15a1536d46b7190ad6fff8ce7 to f2f027ad6a6df617c9f125173da71e44\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   crawl_timestamp          1050 non-null   object \n",
      " 1   product_url              1050 non-null   object \n",
      " 2   product_name             1050 non-null   object \n",
      " 3   product_category_tree    1050 non-null   object \n",
      " 4   pid                      1050 non-null   object \n",
      " 5   retail_price             1049 non-null   float64\n",
      " 6   discounted_price         1049 non-null   float64\n",
      " 7   image                    1050 non-null   object \n",
      " 8   is_FK_Advantage_product  1050 non-null   bool   \n",
      " 9   description              1050 non-null   object \n",
      " 10  product_rating           1050 non-null   object \n",
      " 11  overall_rating           1050 non-null   object \n",
      " 12  brand                    712 non-null    object \n",
      " 13  product_specifications   1049 non-null   object \n",
      "dtypes: bool(1), float64(2), object(11)\n",
      "memory usage: 115.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising attribute to automate the goods classification is the *description* feature. However, gathering text data, we cannot use it directly. Let's see how can we handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:51:29.562050Z",
     "start_time": "2020-10-23T14:51:29.542308Z"
    }
   },
   "source": [
    "Before using any Machine Learning on text data, the latter must be transformed into something an algorithm can digest. This process is called text preprocessing and includes various steps:\n",
    "1. Tokenization &ndash; convert sentences to words;\n",
    "2. Remove unnecessary punctuation, numbers;\n",
    "3. Lower casing &ndash; convert a word to lower case\n",
    "4. Remove stop words &ndash; frequent words such as \"the\", \"a\", \"is\";\n",
    "5. Use *Stemming* or *Lemmatization* to convert a word to its base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is defined as a process to split the text into smaller unit, i.e. tokens. The easiest way is white space tokenization, meaning split the text based on whitespace between two words.  \n",
    "The most used function is `word_tokenize()` from the *NLTK* (Natural Language ToolKit) python library. This function splits tokens based on white space and some punctuation marks like `.` and `'` but **not all** of them. Moreover, the methodology used to split contractions like \"isn't\" depends on the contraction itself and may make the stop words removal process (see section 1.4) really painful. For these reasons, it is much more prefered to use regular expressions (regex) and split the text by keeping alphanumeric characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:57.179866Z",
     "start_time": "2020-11-05T17:35:56.863794Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = (pd.Series(data['description'])\n",
    "            .apply(tokenizer.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers are not of any intererest and consequently must be removed. This can be done by iterating over all tokens and only keeping those that are alphabetic with the python function `isalpha()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:57.254272Z",
     "start_time": "2020-11-05T17:35:57.181409Z"
    }
   },
   "outputs": [],
   "source": [
    "words = pd.Series(np.zeros(len(tokens)), index=data.index)\n",
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [w for w in tokens[i]\n",
    "                     if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two words like Text and text, meaning exactly the same, will be considered as two different words. Consequently, it is highly adviced to convert all word to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:57.328721Z",
     "start_time": "2020-11-05T17:35:57.255781Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [w.lower() for w in words[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stop words* are words that do not contribute to the deeper meaning of the sentence and so, do not really help to distinguish two different documents. Worse, they bring noise and may drop significantly the performance of your model. For this reason, they must be removed.   \n",
    "Stop words usually refer to the **most common** words such as \"and\", \"the\" or \"a\". But there is no *single universal list* of stopwords. The stop words list may change depending on your application.  \n",
    "As for tokenization, NLTK provides a list of common stop words for a variety of languages, such a English. This list can be found in the `stopwords` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:57.548777Z",
     "start_time": "2020-11-05T17:35:57.330079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "# filter out stop words\n",
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [w for w in words.iloc[i]\n",
    "                     if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of stemming or Lemmatization is to reduce words like \"studies\" to a root word (\"studi\") or a common base form (\"study\") respectively. Though it is much more easier to develop a stemmer than lemmatizer (which requires deep linguistics knowledge to build the lemma of each word), the latter is prefered. The noise will be more reduced and so, the results provided more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:35:59.056088Z",
     "start_time": "2020-11-05T17:35:57.551682Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [lemmatizer.lemmatize(w)\n",
    "                     for w in words[i]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
