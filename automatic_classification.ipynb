{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: Develop an automatic classification engine for consumer goods.\n",
    "*Pierre-eloi Ragetly*\n",
    "\n",
    "This project is part of the Data Scientist path proposed by OpenClassrooms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:57.583688Z",
     "start_time": "2020-11-05T17:41:56.946521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(89)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams.update({'axes.titleweight': 'bold',\n",
    "                     'axes.titlesize': 16,\n",
    "                     'axes.labelsize': 14,\n",
    "                     'xtick.labelsize': 12,\n",
    "                     'ytick.labelsize': 12})\n",
    "\n",
    "# Where to save the figures\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    folder_path = os.path.join(\"charts\")\n",
    "    if not os.path.isdir(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    path = os.path.join(\"charts\", fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Natural-Language-Processing-(NLP)\" data-toc-modified-id=\"Natural-Language-Processing-(NLP)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Natural Language Processing (NLP)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-preprocessing\" data-toc-modified-id=\"Text-preprocessing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Text preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Remove-numbers\" data-toc-modified-id=\"Remove-numbers-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Remove numbers</a></span></li><li><span><a href=\"#Lower-casing\" data-toc-modified-id=\"Lower-casing-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Lower casing</a></span></li><li><span><a href=\"#Stop-words-removal\" data-toc-modified-id=\"Stop-words-removal-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Stop words removal</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Lemmatization</a></span></li></ul></li><li><span><a href=\"#Feature-extraction\" data-toc-modified-id=\"Feature-extraction-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-words\" data-toc-modified-id=\"Bag-of-words-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Bag of words</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:57.604228Z",
     "start_time": "2020-11-05T17:41:57.585262Z"
    }
   },
   "outputs": [],
   "source": [
    "data = (pd.read_csv(\"data/Flipkart/flipkart_com-ecommerce_sample_1050.csv\")\n",
    "          .set_index('uniq_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:57.616918Z",
     "start_time": "2020-11-05T17:41:57.609551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1050 entries, 55b85ea15a1536d46b7190ad6fff8ce7 to f2f027ad6a6df617c9f125173da71e44\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   crawl_timestamp          1050 non-null   object \n",
      " 1   product_url              1050 non-null   object \n",
      " 2   product_name             1050 non-null   object \n",
      " 3   product_category_tree    1050 non-null   object \n",
      " 4   pid                      1050 non-null   object \n",
      " 5   retail_price             1049 non-null   float64\n",
      " 6   discounted_price         1049 non-null   float64\n",
      " 7   image                    1050 non-null   object \n",
      " 8   is_FK_Advantage_product  1050 non-null   bool   \n",
      " 9   description              1050 non-null   object \n",
      " 10  product_rating           1050 non-null   object \n",
      " 11  overall_rating           1050 non-null   object \n",
      " 12  brand                    712 non-null    object \n",
      " 13  product_specifications   1049 non-null   object \n",
      "dtypes: bool(1), float64(2), object(11)\n",
      "memory usage: 115.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising attribute to automate the goods classification is the *description* feature. However, gathering text data, we cannot use it directly. Let's see how can we handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:51:29.562050Z",
     "start_time": "2020-10-23T14:51:29.542308Z"
    }
   },
   "source": [
    "Before using any Machine Learning on text data, the latter must be transformed into something an algorithm can digest. This process is called NLP and includes two steps:\n",
    "1. Text Preprocessing\n",
    "2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing, or data nomalization, is a process of converting data from the initial raw form into a format more suitable for further analysis.\n",
    "Since data are text and not numbers, we will not use the same techniques that for a classic data wrangling.  \n",
    "The most common ones used to prepare text data are listed below:\n",
    "- Tokenization &ndash; convert sentences to words;\n",
    "- Remove unnecessary punctuation, numbers;\n",
    "- Lower casing &ndash; convert a word to lower case;\n",
    "- Remove stop words &ndash; frequent words such as \"the\", \"a\", \"is\";\n",
    "- Use *Stemming* or *Lemmatization* to convert a word to its base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is defined as a process to split the text into smaller unit, i.e. tokens. The easiest way is white space tokenization, meaning split the text based on whitespace between two words.  \n",
    "The most used function is `word_tokenize()` from the *NLTK* (Natural Language ToolKit) python library. This function splits tokens based on white space and some punctuation marks like `.` and `'` but **not all** of them. Moreover, the methodology used to split contractions like \"isn't\" depends on the contraction itself and may make the stop words removal process (see section 1.1.4) really painful. For these reasons, it is much more prefered to use regular expressions (regex) and split the text by keeping alphanumeric characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:57.888302Z",
     "start_time": "2020-11-05T17:41:57.618239Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = (pd.Series(data['description'])\n",
    "            .apply(tokenizer.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers are not of any intererest and consequently must be removed. This can be done by iterating over all tokens and only keeping those that are alphabetic with the python function `isalpha()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:57.964861Z",
     "start_time": "2020-11-05T17:41:57.890204Z"
    }
   },
   "outputs": [],
   "source": [
    "words = pd.Series(np.zeros(len(tokens)), index=data.index)\n",
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [w for w in tokens[i]\n",
    "                     if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower casing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two words like Text and text, meaning exactly the same, will be considered as two different words. Consequently, it is highly adviced to convert all word to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:58.041641Z",
     "start_time": "2020-11-05T17:41:57.966275Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [w.lower() for w in words[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stop words* are words that do not contribute to the deeper meaning of the sentence and so, do not really help to distinguish two different documents. Worse, they bring noise and may drop significantly the performance of your model. For this reason, they must be removed.   \n",
    "Stop words usually refer to the **most common** words such as \"and\", \"the\" or \"a\". But there is no *single universal list* of stopwords. The stop words list may change depending on the application.  \n",
    "As for tokenization, NLTK provides a list of common stop words for a variety of languages, such a English. This list can be found in the `stopwords` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:58.265881Z",
     "start_time": "2020-11-05T17:41:58.044130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "# filter out stop words\n",
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [w for w in words.iloc[i]\n",
    "                     if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of stemming or Lemmatization is to reduce words like \"studies\" to a root word (\"studi\") or a canonical form (\"study\") respectively. Though it is much more easier to develop a stemmer than a lemmatizer (which requires deep linguistics knowledge to build the lemma of each word), the latter is prefered. The noise will be more reduced and so, the results provided more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:41:59.825961Z",
     "start_time": "2020-11-05T17:41:58.267352Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(words)):\n",
    "    words.iloc[i] = [lemmatizer.lemmatize(w)\n",
    "                     for w in words[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text processing, words represent **discrete, categorical features**. These kind of features cannot be directly used by a machine learning algorithm. Therefore, text must be encoded into vectors of numbers before. This process is called **feature extraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular and simplest feature extraction technique is likely the **bag-of-words** model. This model counts the occurence of each word within a document. Any information about **the order** or **structure** of words is discarded. That is why actually, it is called a **bag** of words.  \n",
    "The assumption is that **similar documents** have **similar contents** and so, the content may learn something about the meaning of the document to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
